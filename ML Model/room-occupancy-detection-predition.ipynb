{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 1. Load Data","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv('../input/room-occupancy/file.csv')\ndf.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-06-21T20:13:34.950139Z","iopub.execute_input":"2022-06-21T20:13:34.951204Z","iopub.status.idle":"2022-06-21T20:13:34.973543Z","shell.execute_reply.started":"2022-06-21T20:13:34.951168Z","shell.execute_reply":"2022-06-21T20:13:34.972343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`*` By checking the shape of the data, we can obtain the numbers of rows and columns.","metadata":{}},{"cell_type":"code","source":"df.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-21T20:13:34.975588Z","iopub.execute_input":"2022-06-21T20:13:34.975899Z","iopub.status.idle":"2022-06-21T20:13:34.984206Z","shell.execute_reply.started":"2022-06-21T20:13:34.975868Z","shell.execute_reply":"2022-06-21T20:13:34.982785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`*` We also want to know if there is any null-value in our data and the `Dtype` of each column.","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2022-06-21T20:13:34.985609Z","iopub.execute_input":"2022-06-21T20:13:34.986509Z","iopub.status.idle":"2022-06-21T20:13:35.013676Z","shell.execute_reply.started":"2022-06-21T20:13:34.986472Z","shell.execute_reply":"2022-06-21T20:13:35.0127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`*` Checking the statisctic info for the numerical columns is also useful:  \n__Light__ and __CO2__: with relative small `means` and `stds` (standard deviations), they both have large `max` (maximun) value, which indicate that these two columns contain _outliers_.","metadata":{}},{"cell_type":"code","source":"df.describe()","metadata":{"execution":{"iopub.status.busy":"2022-06-21T20:13:35.01711Z","iopub.execute_input":"2022-06-21T20:13:35.01789Z","iopub.status.idle":"2022-06-21T20:13:35.053852Z","shell.execute_reply.started":"2022-06-21T20:13:35.017838Z","shell.execute_reply":"2022-06-21T20:13:35.052804Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Data Visualizations\nLine Plots and Box Plots.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef occupancy_plot(df, cat):\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14,4))\n    \n    fig.suptitle(cat)\n    ax1.plot(np.where(df.Occupancy==1, df[cat], None), label='Occupied')\n    ax1.plot(np.where(df.Occupancy==0, df[cat], None), label='Vacant', ls='--')\n    ax1.grid()\n    ax1.legend()\n    \n    ax2.boxplot([df[cat][df.Occupancy==1], df[cat][df.Occupancy==0], df[cat]])\n    ax2.set_xticklabels(['Occupancy', 'Vacant', 'Overall'])\n\n\nfor i in range(1, 6):\n    occupancy_plot(df, df.columns[i])","metadata":{"execution":{"iopub.status.busy":"2022-06-21T20:13:35.055735Z","iopub.execute_input":"2022-06-21T20:13:35.056157Z","iopub.status.idle":"2022-06-21T20:13:36.894635Z","shell.execute_reply.started":"2022-06-21T20:13:35.056125Z","shell.execute_reply":"2022-06-21T20:13:36.893341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`*` Remove _outliers_ column by column.","metadata":{}},{"cell_type":"code","source":"df2 = df\ndf2 = df2[np.abs(df2.Temperature - df2.Temperature.mean()) <= 3*df2.Temperature.std()]\nprint(\"1. Removing the Outliers on 'Temperature' has reduced the data size from {} to {}.\".format(len(df), len(df2)))\nprint(\"\\n\")\ndf = df2[np.abs(df2.Light - df2.Light.mean()) <= 3*df2.Light.std()]\nprint(\"2. Removing the Outliers on 'Light' has reduced the data size from {} to {}.\".format(len(df2), len(df)))\nprint(\"\\n\")\ndf2 = df[np.abs(df.CO2 - df.CO2.mean()) <= 3*df2.CO2.std()]\nprint(\"3. Removing the Outliers on 'CO2' has reduced the data size from {} to {}.\".format(len(df), len(df2)))\nprint(\"\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-06-21T20:13:36.896341Z","iopub.execute_input":"2022-06-21T20:13:36.896805Z","iopub.status.idle":"2022-06-21T20:13:36.918593Z","shell.execute_reply.started":"2022-06-21T20:13:36.896757Z","shell.execute_reply":"2022-06-21T20:13:36.917187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`*` Let's review the data after the _outliers_ are removed.","metadata":{}},{"cell_type":"code","source":"df2.describe()","metadata":{"execution":{"iopub.status.busy":"2022-06-21T20:13:36.920152Z","iopub.execute_input":"2022-06-21T20:13:36.920897Z","iopub.status.idle":"2022-06-21T20:13:36.958516Z","shell.execute_reply.started":"2022-06-21T20:13:36.920819Z","shell.execute_reply":"2022-06-21T20:13:36.957398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(1, 6):\n    occupancy_plot(df2, df2.columns[i])","metadata":{"execution":{"iopub.status.busy":"2022-06-21T20:13:36.960227Z","iopub.execute_input":"2022-06-21T20:13:36.962273Z","iopub.status.idle":"2022-06-21T20:13:38.801277Z","shell.execute_reply.started":"2022-06-21T20:13:36.962217Z","shell.execute_reply":"2022-06-21T20:13:38.800121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`*` Correleation Heatmap:  \n__Humidity Ratio__ and __Humidity__ are strongly, positivedly correlative.  \n__Light__ is a powerful indicator for __Occupancy__.","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\n\nmask = np.triu(np.ones_like(df2.corr()))\nplt.figure(figsize = (15,8))\nsns.heatmap(df2.corr(),annot=True, fmt=\"1.2f\", mask=mask, cmap=\"YlGnBu\")\nplt.yticks(rotation=0)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-21T20:13:38.802605Z","iopub.execute_input":"2022-06-21T20:13:38.802975Z","iopub.status.idle":"2022-06-21T20:13:39.340908Z","shell.execute_reply.started":"2022-06-21T20:13:38.802929Z","shell.execute_reply":"2022-06-21T20:13:39.339366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Predicting Occupancy Using Classification Algorithms\nThe most common classification algorithms include:  \n__I. Logistic Regression__  \n__II. K Nearest Neighbors (KNN)__  \n__III. Support Vector Machine (SVM)__  \n__IV. Decision Tree__  \n__V. Random Forest__  \n__VI. Naive Bayes__  \n__VII. Gradent Boosting__","metadata":{}},{"cell_type":"markdown","source":"`*` Split the data into training and testing sets and apply the resulting model on the testing set.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\n\nX, Y = df2.iloc[:,1:-1], df2.iloc[:,-1]\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=0)\n\n# Create a dataframe that used to store data from confusion matrix and accuracy \nresult = pd.DataFrame(columns=['Classifier','True Negative', 'False Postive', 'False Negative', 'True Positive', 'Classifier Accuracy'])\n\n\ndef accuracy_vis(xtest, ytest, ypred, predit_proba):\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14,4))\n    \n    # Confusion Matrix Visulation\n    cm = confusion_matrix(ytest, ypred)\n    x_axis_labels = ['Actual Postive', 'Actual Negative']\n    y_axis_labels = ['Predicted Postive', 'Predicted Negative']\n    sns.heatmap(cm, fmt=\".0f\", annot=True, linewidths=.5, ax=ax1, \n                cmap=\"YlGnBu\", xticklabels=x_axis_labels)\n    ax1.set_yticklabels(y_axis_labels, rotation=0, ha='right')\n    \n    # ROC Curve Visulation\n    logit_roc_auc = roc_auc_score(ytest, ypred)\n    fpr, tpr, thresholds = roc_curve(ytest, predit_proba[:,1])\n    ax2.plot(fpr, tpr, label='Logistic Regression (area = {})'.format(round(logit_roc_auc,6)))\n    ax2.plot([0, 1], [0, 1],'r--')\n    ax2.set_xlim([0.0, 1.0])\n    ax2.set_ylim([0.0, 1.05])\n    ax2.set_xlabel('False Positive Rate')\n    ax2.set_ylabel('True Positive Rate')\n    ax2.legend()\n    plt.show()\n    return(confusion_matrix(Y_test, Y_pred).ravel())","metadata":{"execution":{"iopub.status.busy":"2022-06-21T20:13:39.34439Z","iopub.execute_input":"2022-06-21T20:13:39.344747Z","iopub.status.idle":"2022-06-21T20:13:39.364621Z","shell.execute_reply.started":"2022-06-21T20:13:39.344714Z","shell.execute_reply":"2022-06-21T20:13:39.363205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"__I. Logistic Regression__","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\nlr.fit(X_train, Y_train)\nY_pred, lr_score, predit_proba = lr.predict(X_test), lr.score(X_test, Y_test), lr.predict_proba(X_test)\nprint('Accuracy of Logistic Regression Classifier on test set: {:.6f}%'.format(lr_score*100))\ntn, fp, fn, tp = accuracy_vis(X_test, Y_test, Y_pred, predit_proba)\nresult.loc['LR'] = ['Logistic Regression', tn, fp, fn, tp, round(lr_score*100, 6)]","metadata":{"execution":{"iopub.status.busy":"2022-06-21T20:13:39.366379Z","iopub.execute_input":"2022-06-21T20:13:39.366692Z","iopub.status.idle":"2022-06-21T20:13:39.870758Z","shell.execute_reply.started":"2022-06-21T20:13:39.366661Z","shell.execute_reply":"2022-06-21T20:13:39.869656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"__II. K Nearest Neighbors (KNN)__  ","metadata":{}},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=int(np.sqrt(len(X_train))))\nknn.fit(X_train, Y_train)\nY_pred, knn_score, predit_proba  = knn.predict(X_test), knn.score(X_test, Y_test), knn.predict_proba(X_test)\nprint('Accuracy of K Nearest Neighbors Classifier on test set: {:.6f}%'.format(knn_score*100))\ntn, fp, fn, tp = accuracy_vis(X_test, Y_test, Y_pred, predit_proba)\nresult.loc['KNN'] = ['K Nearest Neighbors', tn, fp, fn, tp, round(knn_score*100, 6)]","metadata":{"execution":{"iopub.status.busy":"2022-06-21T20:13:39.872008Z","iopub.execute_input":"2022-06-21T20:13:39.872418Z","iopub.status.idle":"2022-06-21T20:13:40.413793Z","shell.execute_reply.started":"2022-06-21T20:13:39.872385Z","shell.execute_reply":"2022-06-21T20:13:40.412538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"__III. Support Vector Machine (SVM)__ ","metadata":{}},{"cell_type":"code","source":"from sklearn.svm import SVC\n\nsvm = SVC(probability=True)\nsvm.fit(X_train, Y_train)\nY_pred, svm_score, predit_proba = svm.predict(X_test), svm.score(X_test, Y_test), svm.predict_proba(X_test)\nprint('Accuracy of Support Vector Machine Classifier on test set: {:.6f}%'.format(svm_score*100))\ntn, fp, fn, tp = accuracy_vis(X_test, Y_test, Y_pred, predit_proba)\nresult.loc['SVM'] = ['Support Vector Machine', tn, fp, fn, tp, round(svm_score*100, 6)]","metadata":{"execution":{"iopub.status.busy":"2022-06-21T20:13:40.415258Z","iopub.execute_input":"2022-06-21T20:13:40.415995Z","iopub.status.idle":"2022-06-21T20:13:40.982262Z","shell.execute_reply.started":"2022-06-21T20:13:40.415959Z","shell.execute_reply":"2022-06-21T20:13:40.98111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"__IV. Decision Tree__ ","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier()\ndt.fit(X_train, Y_train)\nY_pred, dt_score, predit_proba = dt.predict(X_test), dt.score(X_test, Y_test), dt.predict_proba(X_test)\nprint('Accuracy of Decision Tree Classifier on test set: {:.6f}%'.format(dt_score*100))\ntn, fp, fn, tp = accuracy_vis(X_test, Y_test, Y_pred, predit_proba)\nresult.loc['DT'] = ['Decision Tree', tn, fp, fn, tp, round(dt_score*100, 6)]","metadata":{"execution":{"iopub.status.busy":"2022-06-21T20:13:40.984101Z","iopub.execute_input":"2022-06-21T20:13:40.984461Z","iopub.status.idle":"2022-06-21T20:13:41.44833Z","shell.execute_reply.started":"2022-06-21T20:13:40.984429Z","shell.execute_reply":"2022-06-21T20:13:41.44724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"__V. Random Forest__  ","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier()\nrf.fit(X_train, Y_train)\nY_pred, rf_score, predit_proba = rf.predict(X_test), rf.score(X_test, Y_test), rf.predict_proba(X_test)\nprint('Accuracy of Random Forest Classifier on test set: {:.6f}%'.format(rf_score*100))\ntn, fp, fn, tp = accuracy_vis(X_test, Y_test, Y_pred, predit_proba)\nresult.loc['RF'] = ['Random Forest', tn, fp, fn, tp, round(rf_score*100, 6)]","metadata":{"execution":{"iopub.status.busy":"2022-06-21T20:13:41.45008Z","iopub.execute_input":"2022-06-21T20:13:41.450393Z","iopub.status.idle":"2022-06-21T20:13:42.289509Z","shell.execute_reply.started":"2022-06-21T20:13:41.450363Z","shell.execute_reply":"2022-06-21T20:13:42.288361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"__VI. Naive Bayes__","metadata":{}},{"cell_type":"code","source":"from sklearn.naive_bayes import GaussianNB\n\nnb = GaussianNB()\nnb.fit(X_train, Y_train)\nY_pred, nb_score, predit_proba = nb.predict(X_test), nb.score(X_test, Y_test), nb.predict_proba(X_test)\nprint('Accuracy of Naive Bayes Classifier on test set: {:.6f}%'.format(nb_score*100))\ntn, fp, fn, tp = accuracy_vis(X_test, Y_test, Y_pred, predit_proba)\nresult.loc['NB'] = ['Naive Bayes', tn, fp, fn, tp, round(nb_score*100, 6)]","metadata":{"execution":{"iopub.status.busy":"2022-06-21T20:13:42.2913Z","iopub.execute_input":"2022-06-21T20:13:42.291608Z","iopub.status.idle":"2022-06-21T20:13:42.935418Z","shell.execute_reply.started":"2022-06-21T20:13:42.291577Z","shell.execute_reply":"2022-06-21T20:13:42.934125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"__VII. Gradent Boosting__ ","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n\ngb = GradientBoostingClassifier()\ngb.fit(X_train, Y_train)\nY_pred, gb_score, predit_proba = gb.predict(X_test), gb.score(X_test, Y_test), gb.predict_proba(X_test)\nprint('Accuracy of Gradent Boosting Classifier on test set: {:.6f}%'.format(gb_score*100))\ntn, fp, fn, tp = accuracy_vis(X_test, Y_test, Y_pred, predit_proba)\nresult.loc['GB'] = ['Gradent Boosting', tn, fp, fn, tp, round(gb_score*100, 6)]","metadata":{"execution":{"iopub.status.busy":"2022-06-21T20:13:42.93686Z","iopub.execute_input":"2022-06-21T20:13:42.937227Z","iopub.status.idle":"2022-06-21T20:13:43.656513Z","shell.execute_reply.started":"2022-06-21T20:13:42.937194Z","shell.execute_reply":"2022-06-21T20:13:43.655158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Result","metadata":{}},{"cell_type":"code","source":"result.sort_values('Classifier Accuracy', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-21T20:13:43.657919Z","iopub.execute_input":"2022-06-21T20:13:43.658284Z","iopub.status.idle":"2022-06-21T20:13:43.675365Z","shell.execute_reply.started":"2022-06-21T20:13:43.658252Z","shell.execute_reply":"2022-06-21T20:13:43.673906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}